<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="hDA3AMf31FYDR4O0cr5UGJ7SXCLOmoDXKnQUt4x-X_g"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Human-Friendly Interpretation of a 3D Point Clouds Classifier | Simone Antonelli </title> <meta name="author" content="Simone Antonelli"> <meta name="description" content="Implementation of Input Optimization technique to explain the predictions of PointNet classifier."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, simone, antonelli, simone antonelli, simone antonelli sapienza, simone antonelli uniroma1, simone antonelli cispa, cispa helmholtz, sapienza university of rome, computer science, artificial intelligence, ai, machine learning, deep learning, graph neural network, graph representation learning, few shot learning, graphs, networks"> <meta property="og:site_name" content="Simone Antonelli"> <meta property="og:type" content="website"> <meta property="og:title" content="Simone Antonelli | Human-Friendly Interpretation of a 3D Point Clouds Classifier"> <meta property="og:url" content="https://siantonelli.github.io/projects/PointNetExpl/"> <meta property="og:description" content="Implementation of Input Optimization technique to explain the predictions of PointNet classifier."> <meta property="og:image" content="favicon.svg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Human-Friendly Interpretation of a 3D Point Clouds Classifier"> <meta name="twitter:description" content="Implementation of Input Optimization technique to explain the predictions of PointNet classifier."> <meta name="twitter:image" content="favicon.svg"> <meta name="twitter:site" content="@sntonelli"> <meta name="twitter:creator" content="@sntonelli"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons|Montserrat:wght@400;700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.svg?77b1486887219381d4907e8820ba7b94"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://siantonelli.github.io/projects/PointNetExpl/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Simone</span> Antonelli </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Human-Friendly Interpretation of a 3D Point Clouds Classifier</h1> <p class="post-description">Implementation of Input Optimization technique to explain the predictions of PointNet classifier.</p> </header> <article> <style>img{width:90%}</style> <h1 id="introduction">Introduction</h1> <p>In the last few decades, we have witnessed the great success of Deep Learning in many different tasks, due mainly to the usage of deep neural networks that have demonstrated incredibly generalization performances. However, in order to employ these tools in critical real-life task scenarios, we have to know the reason why a network has predicted a specific result, and considering the trend in designing deeper models to enhance their generalization capability, this reasoning process becomes hard to perform.</p> <p>Nevertheless, Explainability comes to our rescue. Explainability is the research area of Artificial Intelligence whose goal is to reason the behaviour of neural networks given their black-box nature. Indeed, the prediction of a deep neural network is the result of many linear combinations with usually thousands of learnable parameters, followed by a non-linear activation. Therefore, there is no possibility to follow the exact map from the input to the prediction that the network performs. For this reason, ad-hoc methods need to be designed in order to clarify the decision-making process of such networks.</p> <p>Two main directions are carried out in the Explainability field, namely Feature Visualization and Feature Attribution. In the former, once we have chosen a particular neuron (or a group of neurons) of a deep network, the task consists in generating the example that highly activates the selected neuron. While in the latter, the goal is to identify the relevant input features that cause a particular prediction for the network.</p> <figure> <p><img src="/assets/img/visualization.png" alt="Feature Visualization" title="An example of Feature Visualization"> <img src="/assets/img/attribution.png" alt="Feature Attribution" title="An example of Feature Attribution"></p> <figcaption>Visualization of examples of Feature Visualization and Feature Attribution: in the former, the example that excites a selected neuron is emphasizes, whereas in the latter the red pixels are relevant for the prediction of the ‘Dog’ and the green pixels are relevant for the prediction of the ‘Cat’. Figure from <a href="https://distill.pub/2017/feature-visualization/" rel="external nofollow noopener" target="_blank">C. Olah et al., 2017</a></figcaption> </figure> <h1 id="feature-visualization-via-input-optimization">Feature Visualization via Input Optimization</h1> <p>A technique to perform Feature Visualization is called Input Optimization. The idea of such a technique is that: since we know that deep neural networks are differentiable with respect to their input, we can use the derivatives to iteratively tweak the input to make it cause a chosen behaviour of the network. Thus, starting from an input consisting of random noise and using the gradient information, we change the input to increase the activation of a previously selected neuron.</p> <figure> <p><img src="/assets/img/activation-optim.png" alt="Input Optimization" title="An example of Input Optimization"></p> <figcaption>Example of Input Optimization process on an image classifier. Figure from <a href="https://distill.pub/2017/feature-visualization/" rel="external nofollow noopener" target="_blank">C. Olah et al., 2017</a></figcaption> </figure> <p>As suggested by the name, this technique is mathematically formulated as an optimization problem. In particular:</p> <ul> <li>given a model \(M\) with fixed weights \(\Theta\), and</li> <li>chosen a neuron (or a group of neuron) whose activation is indicated by the function \(h(\cdot)\)</li> <li>we look for the input \(\mathbf{X}\) that highly maximizes the activation \(h\)</li> </ul> \[\begin{equation} \mathbf{x}^{\star}=\underset{\mathbf{x}}{\arg \max } \ h\left(\mathcal{M}_{\Theta}(\mathbf{x})\right) \end{equation}\] <h3 id="adversarial-examples">Adversarial examples</h3> <p>Unfortunately, the optimization without any constraint results in a senseless example that highly excites the neuron anyway. These examples can be thought of as adversarial examples. Those might be useful to test the robustness of the neural network, but in order to figure out the representations learned by a neural network, they are useless. Thus, we need to inject some natural structure in the example we are synthesizing, and this can be obtained in three different ways: either we can train a prior and harness it during the optimization, or we can use some regularizer in the formula we are optimizing for, or we can impose some constraints the example may take during the optimization. In this work, we use a learned prior in order to obtain good visualizations.</p> <p>The idea of prior consists of the learning of a model of the real data distribution and then using it during the optimization to try to enforce that. In this work, the prior is a generative model that from a low-dimensionality representation, it returns an example of the real data. In this regard, the prior is trained using an autoencoder framework where just the generator represents the prior, thus only the generator is used during the optimization.</p> <figure> <p><img src="/assets/img/autoencoder.png" alt="Autoencoder" title="Autoencoder architecture"></p> <figcaption>An overview of the autoencoder architecture.</figcaption> </figure> <h1 id="feature-visualization-on-non-euclidean-data">Feature Visualization on non-Euclidean data</h1> <p>Overall, Feature Visualization has led to remarkable insights for neural networks dealing with Euclidean data, in particular with images. Nonetheless, considering the increasing rise of the adoption of neural networks dealing with non-Euclidean data, so data like graphs or meshes where also the input structure is relevant, in this work, we use the Feature Visualization approach to provide explanations of these deep neural networks. Specifically, we deal with 3D point clouds, namely unordered sets of points that are easy-to-acquire, and for this reason, they are implemented in many deep learning tasks, like to represent the surrounded context of an autonomous car.</p> <h3 id="pointnet-classifier">PointNet Classifier</h3> <p>The network we are going to explain is the PointNet classifier (<a href="https://arxiv.org/abs/1612.00593" rel="external nofollow noopener" target="_blank">CR. Qi et al., 2017</a>). In the figure, we can see the simple architecture of such a model. Namely, it is composed of multiple shared fully-connected layers between all the points that compose the input point cloud, followed by a max-pooling operation succeeded by several fully-connected layers. This structure gives the model some relevant properties, like permutation invariance and transformation invariance. In particular, given a point cloud of \(n\) points, this is represented by all the \(n!\) permutations of the points, thus all the permutations need to have the same prediction by the net. To obtain this, the authors use a symmetric function represented by the max-pooling operation that captures an aggregation of the n input points. Moreover, if a rigid transformation is applied to the point cloud the prediction of the network doesn’t have to change. Therefore, to obtain transformation invariance the authors of PointNet have used a little neural network called T-Net that predicts an affine transformation that at a first stage is applied to the input, and then, at a second stage, to the features extracted by an intermediate layer.</p> <figure> <p><img src="/assets/img/pointnet.png" alt="PointNet" title="PointNet architecture"></p> <figcaption>An overview of the PointNet classifier architecture.</figcaption> </figure> <h3 id="training-of-the-prior">Training of the prior</h3> <p>Instead, to train the prior we used a modified autoencoder network called, Adversarial AutoEncoder (<a href="https://arxiv.org/abs/1602.02644" rel="external nofollow noopener" target="_blank">A. Dosovitskiy et al., 2016</a>). This framework is composed of four different networks: a fixed encoder \(E: \mathbb{R}^{N \times 3} \rightarrow \mathbb{R}^{\ell}\) that encodes a given point cloud in a latent space trying to preserve the relevant features of the input point cloud (it is represented by the PointNet classifier truncated at the Global Feature vector); a learnable generator \(G_{\theta}: \mathbb{R}^{\ell} \rightarrow \mathbb{R}^{N \times 3}\) that reconstructs a dataset example from a latent code (its architecture is represented in the following figure); a fixed comparator \(C: \mathbb{R}^{N \times 3} \rightarrow \mathbb{R}^{k}\) that extracts feature vectors from an input point cloud (it is represented by the whole PointNet classifier); and finally, a learnable discriminator \(D_{\phi}: \mathbb{R}^{N \times 3} \rightarrow \mathbb{R}\) which aims at distinguishing the original and generated point clouds (its architecture is represented in the following figure).</p> <figure> <p><img src="/assets/img/generator.png" alt="Generator" title="Generator architecture"></p> <figcaption>The Generator architecture of the Adversarial AutoEncoder.</figcaption> </figure> <figure> <p><img src="/assets/img/discriminator.png" alt="Discriminator" title="Discriminator architecture"></p> <figcaption>The Discriminator architecture of the Adversarial AutoEncoder.</figcaption> </figure> <h4 id="3d-adversarial-autoencoder">3D Adversarial AutoEncoder</h4> <p>Our 3D Adversarial AutoEncoder training, as can be seen from the figure, involves three steps that are performed simultaneously. Firstly, the encoder extracts a latent code from an input point cloud, and the generator reconstructs the original point cloud from the latent code. The similarity between the generated and original point cloud is measured by the chamfer loss. In the second step, the discriminator is fed with the original and generated point clouds, and the adversarial loss returns the confidence scores of the discriminator for each point cloud. Finally, the two-point clouds are given as input to the comparator that extracts the corresponding feature vectors. Those are then compared using a feature loss.</p> <figure> <p><img src="/assets/img/AAE.png" alt="3D-AAE" title="3D-AAE framework"></p> <figcaption>The 3D Adversarial AutoEncoder (3D-AAE) pipeline.</figcaption> </figure> <h4 id="losses">Losses</h4> <p>First of all, the Chamfer loss represents the reconstruction loss of the autoencoder. Let</p> <ul> <li>\(\mathbf{x}\) a point cloud from the dataset \(\mathbf{X}\),</li> <li>\(\mathbf{z}=E(\mathbf{x})\) the latent code obtained from \(\mathbf{x}\), and</li> <li>\(\tilde{\mathbf{x}}=G_{\theta}(\mathbf{z})\) the generated point cloud the Chamfer loss measures the distance between each point in the original point cloud \(\mathbf{x}\) to its nearest neighbour in the generated one \(\tilde{\mathbf{x}}\) and viceversa</li> </ul> \[\begin{equation} \mathcal{L}_{\text {point }}=\sum_{i=1}^{N} \min _{\tilde{\mathbf{x}}_{j} \in \tilde{\mathbf{x}}}\left\|\mathbf{x}_{i}-\tilde{\mathbf{x}}_{j}\right\|_{2}^{2}+\sum_{j=1}^{N} \min _{\mathbf{x}_{i} \in \mathbf{x}}\left\|\mathbf{x}_{i}-\tilde{\mathbf{x}}_{j}\right\|_{2}^{2} \end{equation}\] <p>Second, the feature loss measures the proximity between the feature vectors extracted by the comparator network. In fact, the distance is computed with the Mean Squared Error between the features vector of the original point cloud and the features vector of the generated point cloud.</p> \[\begin{equation} \mathcal{L}_{\text {feat }}=\sum_{\mathbf{x} \in \mathbf{X}}\left\|C\left(G_{\theta}(\mathbf{z})\right)-C(\mathbf{x})\right\|_{2}^{2} \end{equation}\] <p>Finally, the adversarial loss is introduced according to the Wasserstein GAN (<a href="https://arxiv.org/abs/1704.00028" rel="external nofollow noopener" target="_blank">I. Gulrajani et al., 2017</a>) approach. Therefore, the training procedure is performed by alternating updates of generator parameters and the discriminator parameters. The generator is updated following the loss obtained by the sum of the chamfer loss and the feature loss minus the score returned by the discriminator on the generated point cloud</p> \[\begin{equation} \mathcal{L}_{G}=\mathcal{L}_{\text {feat }}+\mathcal{L}_{\text {points }}+\underbrace{\left(-D\left(G_{\theta}(\mathbf{z})\right)\right)}_{\mathcal{L}_{\text {adv }}} \end{equation}\] <p>Note that, the minus sign is owing to the fact that the generator is trying to fool the discriminator, and thus it wants the discriminator to give a higher score for the generated point cloud.</p> <p>By contrast, the discriminator parameters are updated according to this other loss</p> \[\begin{equation} \mathcal{L}_{D}=D_{\Phi}(G(\mathbf{z}))-D_{\Phi}(\mathbf{x})+\underbrace{\lambda_{\mathrm{gp}}\left(\left\|\nabla_{\hat{\mathbf{x}}} D_{\Phi}(\hat{\mathbf{x}})\right\|_{2}-1\right)^{2}}_{\text {gradient penalty term }} \end{equation}\] <p>where the score of the discriminator on the generated point cloud is minimized, the scores on the original point cloud are maximized and additionally, the authors add a gradient penalty term to force the discriminator to approximate a \(1\)-Lipschitz function.</p> <h3 id="input-optimization-procedure">Input optimization procedure</h3> <p>Now, having the trained model to explain, namely the PointNet classifier, and the learned prior, namely the generator of the 3D-AAE, we can continue with the input optimization procedure. For this work, we selected the neuron to activate from the final layer of the PointNet classifier, specifically the class logits layer, thus we expect as output something specific to the class we have selected. To leverage the prior, the input to optimize lies in the latent space of the generator, and since we are going to maximize the activation of a neuron, the updates of the input are performed by making steps in the direction of the steepest ascent, namely the direction indicated by the gradients of the activation with respect to the input.</p> <figure> <p><img src="/assets/img/opt-loop.png" alt="Optimization" title="Input Optimization"></p> <figcaption>Input Optimization pipeline.</figcaption> </figure> <p>An overview of the pipeline of the input optimization process is shown in the previous figure. Starting from the latent code, we extract the point cloud using the generator that is then classified by the PointNet model obtaining a score for each of the considered classes. The orange bold arrow indicates the flow of the gradients that from the selected neuron of the last layer of PointNet goes through the point cloud in input back to the latent code of the Generator, which is then properly updated according to the activation we have obtained. More practically, the latent code is updated in such a way that the point cloud produced by the Generator, increasingly improves the activation of the selected neuron of PointNet responsible for the recognition of a particular class.</p> <h1 id="results">Results</h1> <p>To train and evaluate both the PointNet classifier and the 3D-Adversarial AutoEncoder architecture we use the ModelNet40 dataset. The dataset is composed of ~\(12\) thousand 3D shapes representing general objects. Out of those, \(9.843\) shapes build up the training dataset and \(2.468\) shapes build up the test dataset. Since our models deal with point clouds, the shapes are pre-processed. In particular, \(2.048\) points are uniformly sampled from the surface of each shape and then each obtained point cloud is centred and normalized into a unit sphere.</p> <p>Since no checkpoint was provided by the authors of PointNet, we train the classifier from scratch, achieving an accuracy of ~\(86\)% that is almost the same performance as the original model.</p> <figure> <p><img src="/assets/img/accuracy.png" alt="Accuracy" title="Accuracy trend"></p> <figcaption>Accuracy trend of the PointNet classifier on both the training and test dataset of ModelNet40.</figcaption> </figure> <p>Instead, to evaluate the prior, we look at the reconstruction ability that the generator has on examples it has never seen during the training. in particular, we extracted from the test dataset of ModelNet40 one point cloud for each class and we evaluate how it is reconstructed by the generator as the training of the 3D-AAE goes on. We can see from the figures that the reconstructed point clouds approach the original one, so we can state that the generator has learned to represent a point cloud from a latent code.</p> <figure> <p><img src="/assets/img/airplane.png" alt="Airplane" title="Real Airplane"> <img src="/assets/img/gen-airplane.gif" alt="Gen-Airplane" title="Generated Airplane"></p> <figcaption>Generator performances in reconstructing the class Airplane (right: the original point cloud; left: the generated point cloud).</figcaption> </figure> <figure> <p><img src="/assets/img/lamp.png" alt="Lamp" title="Real Lamp"> <img src="/assets/img/gen-lamp.gif" alt="Gen-Lamp" title="Generated Lamp"></p> <figcaption>Generator performances in reconstructing the class Lamp (right: the original point cloud; left: the generated point cloud)</figcaption> </figure> <figure> <p><img src="/assets/img/bowl.png" alt="Bowl" title="Real Bowl"> <img src="/assets/img/gen-bowl.gif" alt="Gen-Bowl" title="Generated Bowl"></p> <figcaption>Generator performances in reconstructing the class Bowl (right: the original point cloud; left: the generated point cloud)</figcaption> </figure> <figure> <p><img src="/assets/img/car.png" alt="Car" title="Real Car"> <img src="/assets/img/gen-car.gif" alt="Gen-Car" title="Generated Car"></p> <figcaption>Generator performances in reconstructing the class Car (right: the original point cloud; left: the generated point cloud)</figcaption> </figure> <p>To better support this statement, we also look at the latent space, where we can see that the classes are clustered in a meaningful way, namely latent codes representing the features of the same class are close together. Thus, we expect that by sampling a code near to the cluster of a class and then giving it as input to the Generator, we obtain a point cloud according to that class. The latent space represents also the space in which we restrict our search during the optimization process.</p> <figure> <p><img src="/assets/img/latent_space.jpg" alt="Latent space" title="Latent space"></p> <figcaption>Latent space of the 3D Adversarial AutoEncoder.</figcaption> </figure> <h3 id="input-optimization-results">Input Optimization results</h3> <p>As previously mentioned, for the optimization we selected the neuron from the class logits layer, thus in output we expect to visualize the part of the point cloud on which the neuron focuses. In fact, as can be seen from the optimizations of the classes Lamp, Vase and Stool, we obtain point clouds reliable compared with the class for which the neuron we are optimizing for is responsible.</p> <figure> <p><img src="/assets/img/opt-lamp.gif" alt="Opt-Lamp" title="Synthesized Lamp"> <img src="/assets/img/opt-vase.gif" alt="Opt-Vase" title="Synthesized Vase"> <img src="/assets/img/opt-stool.gif" alt="Opt-Stool" title="Synthesized Stool"></p> <figcaption>Synthesized examples for the neurons responsible for the classes Lamp, Vase and Stool respectively.</figcaption> </figure> <p>Unfortunately, one of the limitations of this approach is that not all the visualizations produced are interpretable. In fact, as can be seen from the optimization for the class airplane, what we obtain is something that possibly resembles an airplane (as can be inferred by the two wings we obtain at the end of the optimization), but is not clear as the previous classes. This behaviour is due to the fact that, in general, a neuron can be activated in different directions, and each direction can lead to different parts of the point cloud. To overcome this problem, all the possible directions needs to be investigated but given the high number of neurons that compose a network and considering the infinite directions, it is a hard task to perform as well as time-consuming.</p> <figure> <p><img src="/assets/img/opt-airplane.gif" alt="Opt-Airplane" title="Synthesized Airplane"></p> <figcaption>Synthesized example for the neuron responsible for the classes Airplane.</figcaption> </figure> <h1 id="conclusions">Conclusions</h1> <p>To sum up, in this work we have exhibited an explanation approach working with neural networks dealing with 3D data, in particular with 3D point clouds. Moreover, To make the approach work in this context, we have also proposed a new way to learn a prior leveraging a generative framework named 3D Adversarial AutoEncoder in order to inject some prior for the structure of the synthesised point cloud.</p> <p>Finally, since Explainability of models dealing with non-Euclidean data is still a poorly investigated field, this work can be considered as a starting point for ongoing research in this direction, and for this reason, it can be extended in several ways:</p> <ul> <li>excite different neurons in the middle layers of the models to have more insights into the hidden features the network has learned, but we have also to come out with a reasonable way to represent high-dimensionality data in a 3D space.</li> <li>design novel approaches to learn a prior that is as general as possible and that can be used equally to diverse architectures without the need to train it from scratch every time.</li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Simone Antonelli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-XCK458LS4R"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XCK458LS4R");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>