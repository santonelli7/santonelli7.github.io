<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script id="Cookiebot" src="https://consent.cookiebot.com/uc.js" data-cbid="db78cf14-979d-40d8-a2ac-c8f9eaa1f298" data-blockingmode="auto" type="text/javascript"></script> <meta name="google-site-verification" content="hDA3AMf31FYDR4O0cr5UGJ7SXCLOmoDXKnQUt4x-X_g"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reinforcement Learning for drone networks | Simone Antonelli</title> <meta name="author" content="Simone Antonelli"> <meta name="description" content="Implementation of both routing algorithm and MAC protocol for networks of drones using reinforcement learning."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, simone, antonelli, simone antonelli, simone antonelli sapienza, simone antonelli uniroma1, simone antonelli cispa, cispa helmholtz, sapienza university of rome, computer science, artificial intelligence, ai, machine learning, deep learning, graph neural network, graph representation learning, few shot learning, graphs, networks"> <meta property="og:site_name" content="Simone Antonelli"> <meta property="og:type" content="website"> <meta property="og:title" content="Simone Antonelli | Reinforcement Learning for drone networks"> <meta property="og:url" content="https://siantonelli.github.io/projects/DroNet/"> <meta property="og:description" content="Implementation of both routing algorithm and MAC protocol for networks of drones using reinforcement learning."> <meta property="og:image" content="favicon.svg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Reinforcement Learning for drone networks"> <meta name="twitter:description" content="Implementation of both routing algorithm and MAC protocol for networks of drones using reinforcement learning."> <meta name="twitter:image" content="favicon.svg"> <meta name="twitter:site" content="@sntonelli"> <meta name="twitter:creator" content="@sntonelli"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.svg"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://siantonelli.github.io/projects/DroNet/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Simone </span>Antonelli</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning for drone networks</h1> <p class="post-description">Implementation of both routing algorithm and MAC protocol for networks of drones using reinforcement learning.</p> </header> <article> <h1 id="reinforcement-learning-based-mac-protocol">Reinforcement Learning-based MAC protocol</h1> <p>Consider the case of a network of drones trying to communicate with a depot, whose purpose is to allocate bandwidth for them to send data.</p> <h4 id="reinforcement-learning-setting">Reinforcement Learning setting</h4> <p>In this scenario, the depot will represent the <em>agent</em> while the drones with their information define the <em>environment</em>. Due to the lack of information about the drones accessible by the depot, a <em>stateless</em> (i.e., bandit-like) formulation is used as a baseline.</p> <h5 id="reward-function">Reward function</h5> <p>The reward function is a value function determining the `goodness’ of a decision (action) and implicitly defines the goal of the problem. In this case, a smooth reward function is considered as the RL process converges faster on this kind of function rather than sparse ones.</p> <p>The key idea is that the reward should have high value when the depot chose a drone which has packets to upload and the expired packets are few (possibly 0), while it should have low value (negative in our case) if the depot queries a drone that has no packet to deliver; the more the expired packets, the lower the value of the reward. For additional regularizing effect, the reward is restricted to be in the range \([-1;1]\). Let \(D_t\) be 1 if the drone queried at time \(t-1\) had a packet to deliver, and \(F_t\) the feedback returned from the query. Then,</p> \[\begin{equation*} R(t) = \begin{cases} 1.5 -\frac{1}{1 + e^{-F_t}} &amp; \textit{if } D_t=1 \\ -\frac{1}{1 + e^{-F_t}} &amp; \textit{otherwise} \end{cases} \end{equation*}\] <p>The following shows the trend of the defined reward</p> <figure> <p align="center"><img src="/assets/img/trend-rew.png" alt="Reward function" title="Reward function trend"></p> <figcaption>Reward function trend. Red is for \(D_t=1\), blue for \(D_t=0\)</figcaption> </figure> <h5 id="policies">Policies</h5> <p>Two different policies are considered in this work:</p> <ul> <li>\(\varepsilon\)-greedy policy, where the greedy action (drone) is selected with probability \(1-\varepsilon\) and a random action with probability \(\varepsilon\); in this way, we try to define a reasonable trade-off between <em>exploration</em> and <em>exploitation</em>, which is typically a challenge in Reinforcement Learning; Let \(Q_t(a)\) the estimated value of the action \(a\) at time step \(t\), then</li> </ul> \[\begin{equation*} A_t = \begin{cases} \displaystyle{\underset{a}{\operatorname{argmax}}} \ Q_t(a) &amp;\mbox{wp } 1-\varepsilon \\ \textit{a uniformly random action} &amp;\mbox{wp } \varepsilon \end{cases} \end{equation*}\] <ul> <li>Upper-Confidence Bound} (UCB), which considers how close the estimate of action is to be maximal and the uncertainty in its estimate:</li> </ul> \[\begin{equation*} A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c \sqrt{\frac{ln \ t}{N_t(a)}} \ \right] \end{equation*}\] <p>where \(N_t(a)\) denotes the number of times that action \(a\) has been selected before time \(t\), and \(c &gt; 0\) controls the degree of exploration.</p> <p>The idea of UCB is that the square root term is a measure of the uncertainty in the estimate of \(a\)’s value. Each time \(a\) is selected the uncertainty is presumably reduced since \(N_t(a)\) increments. On the other hand, each time an action other than \(a\) is selected, \(t\) increases but \(N_t(a)\) does not, so the uncertainty estimate increases instead.</p> <h4 id="learning-models">Learning models</h4> <p>In order to face this task, two different RL approaches are considered: due to the bandit-like nature of the problem, k-armed bandit represents the baseline; an improvement is achieved using Gradient Bandit.</p> <p>Finally, the final model is based on Deep Reinforcement Learning, using, in particular, the Double Q-Learning algorithm, which produced the best results altogether.</p> <h5 id="gradient-bandit-algorithm">Gradient Bandit Algorithm</h5> <p>In this setting, the k arms are exactly the drones that the depot can query. The Gradient Bandit algorithm computes preferences of actions \(H\): the larger the preference, the more often the action is taken. Actions are selected accordingly to a softmax distribution \(\pi\) over the preferences. Let \(A_t\) be the action selected at time \(t\) and \(a\) all the actions \(\neq A_t\). \(H\) is then updated in each iteration as:</p> \[\begin{align*} H_{t+1} (A_t) &amp;= H_t(A_t) + \alpha \left(R_t - \bar{R}_t \right) \left( 1 - \pi_t(A_t) \right) \\ H_{t+1} (a) &amp;= H_t(a) + \alpha \left(R_t - \bar{R}_t \right)\pi_t(a) \quad \end{align*}\] <p>where \(\alpha\) is a step-size parameter and \(\bar{R}_t\) is the average of the rewards.</p> <p>Observe that using softmax over the \(H\) values of actions also induces a policy given by the discrete distribution of the softmax values.</p> <h5 id="double-deep-q-network">Double Deep Q-Network</h5> <p>The idea, introduced by <a href="https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf" rel="external nofollow noopener" target="_blank">Mnih et al., 2015</a>, is that deep neural networks being universal function approximators implies they can also approximate Q-functions.</p> <p>However, this category of algorithms relies on a stateful representation: that is, the Q-values of actions are conditioned over the state of the environment in which they are evaluated. So, the environment state at time \(t\) is represented as:</p> <ul> <li>A one-hot vector identifying the drone queried at \(t-1\)</li> <li>The miss ratio for each drone (# queried \(a\) and no packet / # queried \(a\))</li> <li>The hit ratio for each drone (# queried \(a\) and packet / # queried \(a\))</li> <li>The guess ratio for each drone (# queried / \(t\))</li> </ul> <p>For training, the Double DQN algorithm is employed. The idea consists in having two identical networks (one for prediction and the other for evaluation) and only updating the former (the ‘current’ one), while the latter (the ‘target’) has its weights set equal to the former for each \(\tau\) step.</p> <p>Up to now, DQN training algorithms generally only differ in the chosen loss function:</p> \[\begin{align*} &amp; L((s,a,r,s'),\mathbf{\theta},\mathbf{\theta'}) = \\ &amp; (r + \gamma Q(s',\underset{a'}{\operatorname{argmax}} Q(s',a',\mathbf{\theta}),\mathbf{\theta'}) - Q(s,a,\mathbf{\theta}))^2 \end{align*}\] <p>where \(\theta, \theta'\) are the weights of the current and target network, respectively, and \((s,a,r,s')\) represents an experience in which the agent applies \(a\) in \(s\), bringing the environment in state \(s'\), and gets reward \(r\).</p> <p>\(\gamma \in [0,1]\) is a discount factor.</p> <p>In this formulation, it can be seen that the target network evaluates the greedy action computed by the current network, and this term is compared to the Q-prediction of the action that the agent actually took during exploration.</p> <p>We also tested the usage of replay buffers, <em>i.e.</em>, storing experiences in a buffer and sampling them to train the network in batches, but it yielded worse results than just using the most recent experience (probably due to the length of the training, much shorter than the usual reinforcement learning process, which can easily take days).</p> <h4 id="results">Results</h4> <p>Overall, the best results are given by the double DQN, using UCB as policy. \(\varepsilon\)-greedy would yield results slightly higher in metric but would fail to generalize well (<em>e.g.</em> in the gaussian test case, it would just pick the two majority classes and ignore all the others, while UCB would try to approximate the actual distribution).</p> <figure> <p><img src="/assets/img/hit-distr.png" alt="Hit distribution" title="Hit distribution for Gradient Bandit and Double DQN, compared with the ground truth"></p> <figcaption>Hit distribution for Gradient Bandit and Double DQN, compared with the ground truth. Configuration: 15.000 events, 10 drones, Gaussian distribution.</figcaption> </figure> <h1 id="reinforcement-learning-based-routing-protocol">Reinforcement Learning-based Routing protocol</h1> <p>In a patrolling scenario where a drone explores an area, a typical issue is sending packets to the depot only after completing the whole path. This behavior may be undesired since it could be important to receive packets quickly to avoid expiration. This homework aims to tackle this issue by implementing a routing protocol that, leveraging a squad of drones, improves the delivery of packets to the depot. We present two different Q-learning-based approaches: the Q-Table model and Double Deep Q-Network.</p> <h4 id="reinforcement-learning-setting-1">Reinforcement Learning setting</h4> <p>In this setting, the environment is composed of a squad of \(N\) drones and a depot, with a single drone patrolling the area and the others performing various tasks, and they can be exploited to offload packets and reduce data latency at the depot; in particular, <em>drone 0</em> can choose to send, or not, a packet to a close-by drone which is responsible of delivering it to the depot when it can. If <em>drone 0</em> decides to keep a packet, then it will be delivered after completing his route.</p> <p>So, the agent is <em>drone 0</em> while the environment is represented by the remaining ones and the depot.</p> <h5 id="reward-function-1">Reward function</h5> <p>Considering that every time the depot receives a packet, feedback is given to <em>drone 0</em>, and we leverage this feedback to get a value that tells us if the chosen action was good or not. Specifically, the feedback tells us if the packet is delivered correctly or not (respectively \(1\) and \(-1\)), from which drone is delivered and the delay; we use only the first and last information to compute the reward. Let \(d\) be the delay, \(o\) the outcome and \(e\) the delay after which the packet expires (in this case \(e = 2000\)).</p> <p>The following functions, except the constant one, are monotonic decreasing functions since we want that higher delay leads to lower reward:</p> <ul> <li> <p>Logarithmic reward function:</p> \[\begin{equation*} R(t) = \begin{cases} -log(\frac{d}{e}) &amp; \textit{if } o &gt; 0 \\ -10 &amp; \textit{otherwise} \end{cases} \end{equation*}\] </li> <li> <p>Hyperbolic reward function:</p> \[\begin{equation*} R(t) = \begin{cases} \frac{1}{d+1} &amp; \textit{if } o &gt; 0 \\ -1 &amp; \textit{otherwise} \end{cases} \end{equation*}\] </li> <li> <p>Linear reward function:</p> \[\begin{equation*} R(t) = \begin{cases} -\frac{d}{e}+1 &amp; \textit{if} o &gt; 0 \\ -1 &amp; \textit{otherwise} \end{cases} \end{equation*}\] </li> <li> <p>Quadratic reward function:</p> \[\begin{equation*} R(t) = \begin{cases} -(\frac{d}{e})^2+1 &amp; \textit{if } o &gt; 0 \\ -1 &amp; \textit{otherwise} \end{cases} \end{equation*}\] </li> <li> <p>Constant reward function:</p> \[\begin{equation*} R(t) = \begin{cases} 1 &amp; \textit{if } o &gt; 0 \\ -1 &amp; \textit{otherwise} \end{cases} \end{equation*}\] </li> </ul> <figure> <p><img src="/assets/img/rew_comp.png" alt="Reward trend" title="Reward function trend"></p> <figcaption>Reward function trend. Blue and purple functions have similar curves, but the latter approach 0 much faster. Purple, green, and yellow functions range from 0 to 1. The hyperbolic function is the one which approaches 0 fastest, while the quadratic function approaches 0 slowest.</figcaption> </figure> <figure> <p><img src="/assets/img/rew-conv.png" alt="Reward convergence" title="Reward convergence comparison"></p> <figcaption>Reward convergence for Q-Table model and Double DQN. The graph shows how Q-Table not only outperforms the DQN approach (as it gets a higher average reward) but also converges a lot earlier (around 30k steps, while DQN requires at least 80k).</figcaption> </figure> <h5 id="policy">Policy</h5> <p>A typical challenge in RL is to define a trade-off between exploration and exploitation to make the model able to look for new good solutions and not only exploit greedy ones.</p> <p>The \(\varepsilon\)-greedy policy is the easiest way to implement this trade-off; in particular, with probability \((1 - \varepsilon)\) it returns the greedy action, while with probability \(\varepsilon\) it returns a random action, namely the model explores the action space:</p> \[\begin{equation*} A_t = \begin{cases} \displaystyle{\underset{a}{\operatorname{argmax}}} \ Q_t(a) &amp;\mbox{wp } 1-\varepsilon \\ \textit{a uniformly random action} &amp;\mbox{wp } \varepsilon \end{cases} \end{equation*}\] <h4 id="learning-models-1">Learning models</h4> <p>The two approaches implemented to solve the task are:</p> <ul> <li>Q-Table model;</li> <li>Double Deep Q-Learning.</li> </ul> <h5 id="q-table-model">Q-Table model</h5> <p>We define the state space as follows:</p> \[\begin{equation*} S = H \times I \end{equation*}\] <p>where:</p> <ul> <li>\(h \in H\) is the number of hops to the depot from the last reached target;</li> <li>\(i \in I\) be the index of the cell where <em>drone 0</em> is located.</li> </ul> <p>We define the action space as follows:</p> \[\begin{equation*} A =\{d | d \textit{ is a drone}\} \end{equation*}\] <p>Taking action \(d\), <em>drone 0</em> will use <em>drone d</em> as relay if \(d\) is another drone and keep the packet if \(d\) is itself.</p> <p>We also need to define the successor \(s'\) of a state \(s\) given a certain action \(a\).</p> \[\begin{equation*} s' = (h', i') \end{equation*}\] <p>Let \(t\) be the next target which the drone has to reach when it takes the action $a$ in state \(s\), then:</p> <ul> <li>\(h'\) is the number of hops from \(t\) to the depot;</li> <li>\(i'\) is the cell index of \(t\).</li> </ul> <p>To train the model the classic Q-Table algorithm is used, using the following update rule:</p> \[\begin{equation*} \scriptstyle Q(s,a) \leftarrow Q(s,a) + \alpha\left[R + \gamma \displaystyle{\underset{a' \in A}{\operatorname{argmax}}} Q(s', a') - Q(s, a) \right] \end{equation*}\] <h5 id="double-deep-q-network-1">Double Deep Q-Network</h5> <p>In this case, the environment state at time \(t\) is represented as follows:</p> <ul> <li>Coordinates of each neighbor drone (\((0,0)\) for non-neighboring drones);</li> <li>Distance of each neighbor drone from the depot;</li> <li>Coordinates and distance from the depot of drone \(0\);</li> <li>Current step in the route of drone \(0\) (as a one-hot vector).</li> </ul> <p>For training, the Double DQN algorithm is used. Now, the loss is computed as:</p> \[\begin{align*} &amp; L((s,a,r,s'),\mathbf{\theta},\mathbf{\theta'}) = \\ &amp; (r + \gamma Q(s',\displaystyle{\underset{a'}{\operatorname{argmax}}} Q(s',a',\mathbf{\theta}),\mathbf{\theta'}) - Q(s,a,\mathbf{\theta}))^2 \end{align*}\] <p>Where $\theta, \theta’$ are the weights of the current and target network, respectively, and \((s,a,r,s')\) represents an experience in which the agent applies \(a\) in \(s\), bringing the environment in state \(s'\), and gets reward \(r\).</p> <p>\(\gamma \in [0,1]\) is a discount factor.</p> <p>In this formulation, it can be seen that the target network evaluates the greedy action computed by the current network, and this term is compared to the Q-prediction of the action that the agent actually took during exploration.</p> <p>Furthermore, we used a <em>replay buffer</em> to achieve a regularizing effect. The idea is to store past experiences in a buffer and sample \(N\) to train the network in batches. Ideally, this should allow the model not to `forget’ past experiences. This technique produced an observable performance increase.</p> <h4 id="results-1">Results</h4> <p>Despite Deep RL giving impressive results in recent works, this setting is probably not the best for such models; in particular, the Q-Table model would consistently outperform the DQN, also achieving faster convergence time. The main motivation behind this result is the number of updates. In particular, the models receive an update for each delivered or expired packet. This number is a lot smaller than the length of the simulation (we get ~30 updates for 18k steps of simulation) and simply not enough to train a neural network well. For this reason, the size of the network has to be limited (the final model has ~800k weights), as a bigger network would achieve better results but would require even more time to converge. This trade-off between expressive power and convergence time led to non-optimal results.</p> <figure> <p><img src="/assets/img/ratio_QT.png" alt="QT Delivery ratio" title="Delivery ratio of the Q-Table model"> <img src="/assets/img/ratio_NN.png" alt="NN Delivery ratio" title="Delivery ratio of the neural network"></p> <figcaption>A comparison between the ratio delivery detected of the models.</figcaption> </figure> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Simone Antonelli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: October 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-XCK458LS4R"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XCK458LS4R");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>